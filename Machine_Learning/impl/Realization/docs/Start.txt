Java Machine Learning Frameworks
    Encog (Multicore and GPU powered)
        http://www.heatonresearch.com/encog/
    Neuroph(Lightweight and easy to use)
        http://neuroph.sourceforge.net/index.html
        http://neuroph.sourceforge.net/documentation.html

Neural Netwrok(NN) Steps
------------------------
1. Create Perceptron network
2. Create training set
3. Train network
4. Test trained network 
5. Save tested network
6. Use tested network for future data

We can implement above 1-5 steps using Java Neuroph Studio or Java neuroph library.


NN Types(org.neuroph.nnet)
---------------------------
ADALINE("Adaline")
PERCEPTRON("Perceptron")
MULTI_LAYER_PERCEPTRON("Multi Layer Perceptron")
HOPFIELD("Hopfield")
KOHONEN("Kohonen")
NEURO_FUZZY_REASONER("Neuro Fuzzy Reasoner")
SUPERVISED_HEBBIAN_NET("Supervised Hebbian network")
UNSUPERVISED_HEBBIAN_NET("Unsupervised Hebbian network")
COMPETITIVE("Competitive")
MAXNET("Maxnet")
INSTAR("Instar")
OUTSTAR("Outstar")
RBF_NETWORK("RBF Network")
BAM("BAM")
BOLTZMAN("Boltzman")
COUNTERPROPAGATION("CounterPropagation")
INSTAR_OUTSTAR("InstarOutstar")
PCA_NETWORK("PCANetwork")
RECOMMENDER("Recommender")

Neuron Types(org.neuroph.nnet.comp)
-----------------------------------
Neuron properties
"inputFunction": eg. WeightedSum
"transferFunction": eg. Linear
"neuronType": eg. Neuron
"useBias": eg. yes/no

ThresholdNeuron
CompetitiveNeuron
DelayedNeuron
InputNeuron
InputOutputNeuron
ThresholdNeuron


Neuron InputFunctions(org.neuroph.core.input)
---------------------------------------------
And --	Performs logic AND operation on input vector.
Difference -- Performs the vector difference operation on input and weight vector.
InputFunction -- Neuron's input function.
Max	-- Performs max function on input vector
Min	-- Performs min function on input vector
Or	-- Performs logic OR operation on input vector.
Product -- Performs multiplication of all input vector elements.
Sum	-- Performs summing of all input vector elements.
SumSqr -- Calculates squared sum of all input vector elements.
WeightedSum -- Optimized version of weighted input function


Neuron Transform Functions(org.neuroph.util)
---------------------------------------------
A transfer/activation function, which takes the weighted sum (i.e.output of input function) as input and calculates the output
of the neuron using a simple step (i.e. any one of below) function.
eg. if the weighted sum is greater then zero, the function outputs 1; otherwise, it outputs 0.

LINEAR("Linear") -- slop * net (`slop`: of linear function and `net`: given by input function) : y=mx
RAMP("Ramp")  -- net < this.xLow?this.yLow:(net > this.xHigh?this.yHigh:this.slope * net)
STEP("Step") -- net > 0.0D?this.yHigh:this.yLow
SIGMOID("Sigmoid") -- net > 100.0D?1.0D:net < -100.0D?0.0D:double den = 1.0D + Math.exp(-this.slope * net);output = 1.0D / den;return this.output;
TANH("Tanh") --
GAUSSIAN("Gaussian") -- Math.exp(-Math.pow(net, 2.0D) / (2.0D * Math.pow(this.sigma, 2.0D)));
TRAPEZOID("Trapezoid") -- net >= this.leftHigh && net <= this.rightHigh?1.0D:(net > this.leftLow && net < this.leftHigh?(net - this.leftLow) / (this.leftHigh - this.leftLow):(net > this.rightHigh && net < this.rightLow?(this.rightLow - net) / (this.rightLow - this.rightHigh):0.0D))
SGN("Sgn") -- net > 0.0D?1.0D:-1.0D
SIN("Sin") -- Math.sin(net)
LOG("Log") -- Math.log(net)


Learning Types(org.neuroph.core.learning)
-----------------------------------------
IterativeLearning
SupervisedLearning
LMS
    SigmoidDeltaRule
        BackPropagation
        MomentumBackPropagation
    SupervisedHebbianLearning
UnsupervisedLearning
    CompetitiveLearning
    UnsupervisedHebbianLearning
        OjaLearning




Some real-world applications of Neuroph
---------------------------------------
- Support for supervised and unsupervised learning rules
- An easy-to-follow structure and logic.
- Java & Neural Network IDE, Neuroph Studio, based on NetBeans Platform
- Image recognition support
- OCR support
- Stock market prediction sample
- Learning vizualisation samples
- Data normalization
- Simple microbenchmarking framework

Neuron Network Event Types
--------------------------
CALCULATED
LAYER_ADDED
LAYER_REMOVED
NEURON_ADDED
NEURON_REMOVED
CONNECTION_ADDED
CONNECTION_REMOVED



Data Normalization Techniques
=============================
The main goal of data normalization is to guarantee the quality of the data before it is fed to any learning algorithm.

It can be used to scale the data in the same range of values for each input feature in order to minimize bias within the
neural network for one feature to another.

Data normalization can also speed up training time by starting the training process for each feature within the same scale.

It is especially useful for modeling application where the inputs are generally on widely different scales.

Different techniques can use different rules such as
1. min-max
2. Z-score
3. decimal scaling
4. Median normalization
5. etc..etc..
For more explanation -- set Papers/IJCISIM_24





















